{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2CmceROMrbD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23956,
     "status": "ok",
     "timestamp": 1619030509091,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "a2CmceROMrbD",
    "outputId": "3c9db06a-69d7-4ff1-e502-8a58e4f2a5dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "described-manual",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8171,
     "status": "ok",
     "timestamp": 1619030518121,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "described-manual",
    "outputId": "36812c29-4b64-4f97-e713-d31564af2db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 4.9MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (20.9)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.8.9)\n",
      "Collecting terminaltables\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
      "Collecting colorama\n",
      "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.22.2.post1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras-tuner) (1.0.1)\n",
      "Building wheels for collected packages: keras-tuner, terminaltables\n",
      "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=59bac5bf9387a62cea0f3d25e004354e8b5a94cb673c09790dd01463a6bce6a5\n",
      "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
      "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=0adb72e027ee90f54f0654cff9846019c7d84a547d55f352dbb7afd666251aa2\n",
      "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
      "Successfully built keras-tuner terminaltables\n",
      "Installing collected packages: terminaltables, colorama, keras-tuner\n",
      "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random, os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ModelCheckpoint\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "!pip install keras-tuner\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "HkOODyICMaLe",
   "metadata": {
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1619030525036,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "HkOODyICMaLe"
   },
   "outputs": [],
   "source": [
    "PATH=\"/content/drive/MyDrive/SEM2/BD/Stonks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "valued-viewer",
   "metadata": {
    "executionInfo": {
     "elapsed": 1291,
     "status": "ok",
     "timestamp": 1619032676983,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "valued-viewer"
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
    "RATIO_TO_PREDICT = \"currentVal\"\n",
    "EPOCHS = 50  # how many passes through our data\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "psychological-marsh",
   "metadata": {
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1619030536742,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "psychological-marsh"
   },
   "outputs": [],
   "source": [
    "def classify(current, future):\n",
    "    if float(future) > float(current):  # if the future price is higher than the current, that's a buy, or a 1\n",
    "        return 1\n",
    "    else:  # otherwise... it's a 0!\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quick-integrity",
   "metadata": {
    "executionInfo": {
     "elapsed": 704,
     "status": "ok",
     "timestamp": 1619030537415,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "quick-integrity"
   },
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interracial-chorus",
   "metadata": {
    "executionInfo": {
     "elapsed": 1154,
     "status": "ok",
     "timestamp": 1619030539062,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "interracial-chorus"
   },
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
    "\n",
    "    for col in df.columns:  # go through all of the columns\n",
    "        if col != \"target\":  # normalize all ... except for the target itself!\n",
    "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
    "            df=clean_dataset(df) # remove the nas created by pct_change\n",
    "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
    "\n",
    "    df.dropna(inplace=True)  # cleanup again... jic.\n",
    "\n",
    "\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "\n",
    "    for i in df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
    "\n",
    "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "\n",
    "    buys = []  # list that will store our buy sequences and targets\n",
    "    sells = []  # list that will store our sell sequences and targets\n",
    "\n",
    "    for seq, target in sequential_data:  # iterate over the sequential data\n",
    "        if target == 0:  # if it's a \"not buy\"\n",
    "            sells.append([seq, target])  # append to sells list\n",
    "        elif target == 1:  # otherwise if the target is a 1...\n",
    "            buys.append([seq, target])  # it's a buy!\n",
    "\n",
    "    random.shuffle(buys)  # shuffle the buys\n",
    "    random.shuffle(sells)  # shuffle the sells!\n",
    "\n",
    "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
    "\n",
    "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
    "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
    "\n",
    "    sequential_data = buys+sells  # add them together\n",
    "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:  # going over our new sequential data\n",
    "        X.append(seq)  # X is the sequences\n",
    "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
    "\n",
    "    return np.array(X), y  # return X and y...and make X a numpy array!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "julian-layer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "executionInfo": {
     "elapsed": 2865,
     "status": "ok",
     "timestamp": 1619030543387,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "julian-layer",
    "outputId": "a3fb4093-569e-47d0-988a-d1f69b3cd833"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currentVal</th>\n",
       "      <th>volume</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1546314360</th>\n",
       "      <td>10875.750000</td>\n",
       "      <td>724.6</td>\n",
       "      <td>4.700195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546314420</th>\n",
       "      <td>10875.549805</td>\n",
       "      <td>724.6</td>\n",
       "      <td>2.150391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546314480</th>\n",
       "      <td>10865.049805</td>\n",
       "      <td>724.6</td>\n",
       "      <td>13.849609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546314540</th>\n",
       "      <td>10844.650391</td>\n",
       "      <td>724.6</td>\n",
       "      <td>19.149414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546314600</th>\n",
       "      <td>10841.849609</td>\n",
       "      <td>724.6</td>\n",
       "      <td>7.449219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              currentVal  volume       diff\n",
       "time                                       \n",
       "1546314360  10875.750000   724.6   4.700195\n",
       "1546314420  10875.549805   724.6   2.150391\n",
       "1546314480  10865.049805   724.6  13.849609\n",
       "1546314540  10844.650391   724.6  19.149414\n",
       "1546314600  10841.849609   724.6   7.449219"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df=pd.read_csv(PATH+\"/Nifty50.csv\",index_col='time')\n",
    "main_df['diff']=main_df.high-main_df.low\n",
    "main_df=main_df.drop(labels=['open','high','low'],axis=1)\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fatal-reverse",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1619030564191,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "fatal-reverse",
    "outputId": "20a4214e-129d-4be9-9ebe-66309e7b6981"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([102912, 104018]))"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
    "main_df.dropna(inplace=True)\n",
    "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}'], main_df['future']))\n",
    "main_df.dropna(inplace=True)\n",
    "np.unique(main_df['target'],return_counts=True) #OMG! almost balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fifth-taste",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27679,
     "status": "ok",
     "timestamp": 1619030594059,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "fifth-taste",
    "outputId": "a8d31917-6ae6-4c37-8fd7-0e272b6699b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 143608 validation: 60992\n",
      "Dont buys: 71804, buys: 71804\n",
      "VALIDATION Dont buys: 30496, buys: 30496\n"
     ]
    }
   ],
   "source": [
    "## here, split away some slice of the future data from the main main_df.\n",
    "times = sorted(main_df.index.values)\n",
    "last_5pct = sorted(main_df.index.values)[-int(0.3*len(times))]\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
    "main_df = main_df[(main_df.index < last_5pct)]\n",
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)\n",
    "\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tF6HV1gEGmWk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11390,
     "status": "ok",
     "timestamp": 1619037034012,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "tF6HV1gEGmWk",
    "outputId": "cc471037-99d7-4bb6-ecfa-df102069824a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 100542 validation: 21008, test: 21346\n",
      "TRAIN Dont buys: 50271, buys: 50271\n",
      "VALIDATION Dont buys: 10504, buys: 10504\n",
      "TEST Dont buys: 10673, buys: 10673\n"
     ]
    }
   ],
   "source": [
    "## here, split away some slice of the future data from the main main_df.\n",
    "times = sorted(main_df.index.values)\n",
    "last_5pct = sorted(main_df.index.values)[-int(0.3*len(times))]\n",
    "validation_main_df_buff = main_df[(main_df.index >= last_5pct)]\n",
    "main_df = main_df[(main_df.index < last_5pct)]\n",
    "train_x, train_y = preprocess_df(main_df)\n",
    "\n",
    "times = sorted(validation_main_df_buff.index.values)\n",
    "last_5pct = sorted(validation_main_df_buff.index.values)[-int(0.5*len(times))]\n",
    "\n",
    "validation_main_df = validation_main_df_buff[(validation_main_df_buff.index >= last_5pct)]\n",
    "test_df = validation_main_df_buff[(validation_main_df_buff.index < last_5pct)]\n",
    "\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)\n",
    "test_x, test_y = preprocess_df(test_df)\n",
    "\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}, test: {len(test_x)}\")\n",
    "print(f\"TRAIN Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")\n",
    "print(f\"TEST Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BOAmL1loysdw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1616936889045,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "BOAmL1loysdw",
    "outputId": "11e71078-e6f1-4e73-d2a8-651b9f48eea7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.system(\"rm -rf \"+PATH+\"/LOGS_2/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-annual",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27859,
     "status": "ok",
     "timestamp": 1619030661198,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "referenced-annual",
    "outputId": "6bea0b73-f0dd-403b-de89-fe928f041044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project /content/drive/MyDrive/SEM2/BD/Stonks/LOGS_2/untitled_project/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from /content/drive/MyDrive/SEM2/BD/Stonks/LOGS_2/untitled_project/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 9\n",
      "input_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "drop_out (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "n_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2, 'step': 1, 'sampling': None}\n",
      "lstm_0_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "drop_0_units (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "hidden_dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 32, 'step': 2, 'sampling': None}\n",
      "drop_5_out (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "lstm_1_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "drop_1_units (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in /content/drive/MyDrive/SEM2/BD/Stonks/LOGS_2/untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 32\n",
      "drop_out: 0.25000000000000006\n",
      "n_layers: 1\n",
      "lstm_0_units: 64\n",
      "drop_0_units: 0.15000000000000002\n",
      "hidden_dense_units: 22\n",
      "drop_5_out: 0.3500000000000001\n",
      "lstm_1_units: 192\n",
      "drop_1_units: 0.3500000000000001\n",
      "Score: 0.5448583364486694\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 96\n",
      "drop_out: 0.20000000000000004\n",
      "n_layers: 1\n",
      "lstm_0_units: 64\n",
      "drop_0_units: 0.15000000000000002\n",
      "hidden_dense_units: 20\n",
      "drop_5_out: 0.20000000000000004\n",
      "lstm_1_units: 160\n",
      "drop_1_units: 0.30000000000000004\n",
      "Score: 0.5360702872276306\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 96\n",
      "drop_out: 0.15000000000000002\n",
      "n_layers: 1\n",
      "lstm_0_units: 64\n",
      "drop_0_units: 0.3500000000000001\n",
      "hidden_dense_units: 10\n",
      "drop_5_out: 0.15000000000000002\n",
      "lstm_1_units: 64\n",
      "drop_1_units: 0.20000000000000004\n",
      "Score: 0.534660279750824\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 64\n",
      "drop_out: 0.3500000000000001\n",
      "n_layers: 2\n",
      "lstm_0_units: 64\n",
      "drop_0_units: 0.45000000000000007\n",
      "hidden_dense_units: 18\n",
      "drop_5_out: 0.1\n",
      "lstm_1_units: 192\n",
      "drop_1_units: 0.3500000000000001\n",
      "Score: 0.5319550037384033\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 160\n",
      "drop_out: 0.25000000000000006\n",
      "n_layers: 1\n",
      "lstm_0_units: 64\n",
      "drop_0_units: 0.30000000000000004\n",
      "hidden_dense_units: 30\n",
      "drop_5_out: 0.15000000000000002\n",
      "lstm_1_units: 256\n",
      "drop_1_units: 0.45000000000000007\n",
      "Score: 0.5310368537902832\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 160\n",
      "drop_out: 0.30000000000000004\n",
      "n_layers: 1\n",
      "lstm_0_units: 64\n",
      "drop_0_units: 0.20000000000000004\n",
      "hidden_dense_units: 26\n",
      "drop_5_out: 0.20000000000000004\n",
      "lstm_1_units: 160\n",
      "drop_1_units: 0.3500000000000001\n",
      "Score: 0.5291185975074768\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 160\n",
      "drop_out: 0.1\n",
      "n_layers: 1\n",
      "lstm_0_units: 96\n",
      "drop_0_units: 0.45000000000000007\n",
      "hidden_dense_units: 20\n",
      "drop_5_out: 0.30000000000000004\n",
      "lstm_1_units: 256\n",
      "drop_1_units: 0.5000000000000001\n",
      "Score: 0.5291185975074768\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 128\n",
      "drop_out: 0.25000000000000006\n",
      "n_layers: 1\n",
      "lstm_0_units: 128\n",
      "drop_0_units: 0.5000000000000001\n",
      "hidden_dense_units: 14\n",
      "drop_5_out: 0.25000000000000006\n",
      "lstm_1_units: 224\n",
      "drop_1_units: 0.45000000000000007\n",
      "Score: 0.5291022062301636\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 128\n",
      "drop_out: 0.20000000000000004\n",
      "n_layers: 2\n",
      "lstm_0_units: 128\n",
      "drop_0_units: 0.15000000000000002\n",
      "hidden_dense_units: 16\n",
      "drop_5_out: 0.40000000000000013\n",
      "lstm_1_units: 64\n",
      "drop_1_units: 0.15000000000000002\n",
      "Score: 0.5290365815162659\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 32\n",
      "drop_out: 0.5000000000000001\n",
      "n_layers: 1\n",
      "lstm_0_units: 128\n",
      "drop_0_units: 0.30000000000000004\n",
      "hidden_dense_units: 20\n",
      "drop_5_out: 0.20000000000000004\n",
      "lstm_1_units: 32\n",
      "drop_1_units: 0.25000000000000006\n",
      "Score: 0.5290365815162659\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp):  # random search passes this hyperparameter() object \n",
    "    Fleg=True\n",
    "    Fleg1=True\n",
    "    if len(range(hp.Int('n_layers', 0, 2)))==0:\n",
    "        Fleg1=False\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('input_units',\n",
    "                            min_value=32,\n",
    "                            max_value=256,\n",
    "                            step=32), input_shape=(train_x.shape[1:]), return_sequences=Fleg1))\n",
    "    model.add(Dropout(hp.Float('drop_out',\n",
    "                                min_value=0.1,\n",
    "                                max_value=0.5,\n",
    "                                step=0.05)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    for i in range(hp.Int('n_layers', 0, 2)):  # adding variation of layers.\n",
    "\n",
    "        if len(range(hp.Int('n_layers', 0, 2)))==1:\n",
    "            Fleg=False\n",
    "\n",
    "        if i==1:\n",
    "            Fleg=False\n",
    "        model.add(LSTM(hp.Int(f'lstm_{i}_units',\n",
    "                                min_value=32,\n",
    "                                max_value=256,\n",
    "                                step=32),return_sequences=Fleg))\n",
    "        model.add(Dropout(hp.Float(f'drop_{i}_units',\n",
    "                                min_value=0.1,\n",
    "                                max_value=0.5,\n",
    "                                step=0.05)))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "   \n",
    "\n",
    "    model.add(Dense(hp.Int('hidden_dense_units',\n",
    "                    min_value=10,\n",
    "                    max_value=32,\n",
    "                    step=2), activation='relu'))\n",
    "    model.add(Dropout(hp.Float('drop_5_out',\n",
    "                                min_value=0.1,\n",
    "                                max_value=0.5,\n",
    "                                step=0.05)))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=50,  # how many variations on model?\n",
    "    executions_per_trial=1,  # how many trials per variation? (same model could perform differently)\n",
    "    directory=PATH+\"/LOGS_2\")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(x=train_x,\n",
    "             y=np.array(train_y),\n",
    "             epochs=5,\n",
    "             batch_size=BATCH_SIZE,\n",
    "             validation_data=(validation_x, np.array(validation_y)))\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "\n",
    "with open(f\"tuner_{int(time.time())}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tuner, f)\n",
    "\n",
    "# # Score model\n",
    "# score = model.evaluate(validation_x, np.array(validation_y), verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])\n",
    "# # Save model\n",
    "# model.save(PATH+\"/models/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G6YXqN4Q5n7q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1748,
     "status": "ok",
     "timestamp": 1616950671210,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "G6YXqN4Q5n7q",
    "outputId": "8a3645e4-b2f0-431c-a183-a356d75f0d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tuner_1616950324.pkl' -> '/content/drive/MyDrive/SEM2/BD/Stonks/tuner_1616950324.pkl'\n"
     ]
    }
   ],
   "source": [
    "!cp -rv tuner* $PATH/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LhS1GN-88ihQ",
   "metadata": {
    "id": "LhS1GN-88ihQ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "tuner = pickle.load(open(\"/content/drive/MyDrive/SEM2/BD/Stonks/tuner_1616950324.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mQmH6Uw78ufG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4549,
     "status": "ok",
     "timestamp": 1617725766726,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "mQmH6Uw78ufG",
    "outputId": "c69cf8e2-5e69-4099-adeb-fa748e70b78b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 120, 32)           4608      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 120, 32)           128       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 22)                1430      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 46        \n",
      "=================================================================\n",
      "Total params: 31,300\n",
      "Trainable params: 31,108\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "tuner.get_best_models()[0].summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "F2AS4jxCLJr4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153762,
     "status": "ok",
     "timestamp": 1619038006713,
     "user": {
      "displayName": "2020 18010",
      "photoUrl": "",
      "userId": "14677844295391768051"
     },
     "user_tz": -330
    },
    "id": "F2AS4jxCLJr4",
    "outputId": "d31ab24f-e6a6-4df7-81cd-1a720db5fcf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1571/1571 [==============================] - 18s 10ms/step - loss: 0.7407 - accuracy: 0.5097 - val_loss: 0.6916 - val_accuracy: 0.5242\n",
      "Epoch 2/10\n",
      "1571/1571 [==============================] - 15s 9ms/step - loss: 0.6920 - accuracy: 0.5235 - val_loss: 0.6909 - val_accuracy: 0.5270\n",
      "Epoch 3/10\n",
      "1571/1571 [==============================] - 15s 9ms/step - loss: 0.6915 - accuracy: 0.5276 - val_loss: 0.6915 - val_accuracy: 0.5219\n",
      "Epoch 4/10\n",
      "1571/1571 [==============================] - 15s 9ms/step - loss: 0.6906 - accuracy: 0.5295 - val_loss: 0.6921 - val_accuracy: 0.5327\n",
      "Epoch 5/10\n",
      "1571/1571 [==============================] - 14s 9ms/step - loss: 0.6905 - accuracy: 0.5286 - val_loss: 0.6895 - val_accuracy: 0.5327\n",
      "Epoch 6/10\n",
      "1571/1571 [==============================] - 14s 9ms/step - loss: 0.6887 - accuracy: 0.5358 - val_loss: 0.6857 - val_accuracy: 0.5432\n",
      "Epoch 7/10\n",
      "1571/1571 [==============================] - 15s 9ms/step - loss: 0.6861 - accuracy: 0.5434 - val_loss: 0.6853 - val_accuracy: 0.5437\n",
      "Epoch 8/10\n",
      "1571/1571 [==============================] - 15s 9ms/step - loss: 0.6843 - accuracy: 0.5513 - val_loss: 0.6841 - val_accuracy: 0.5514\n",
      "Epoch 9/10\n",
      "1571/1571 [==============================] - 15s 9ms/step - loss: 0.6837 - accuracy: 0.5558 - val_loss: 0.6804 - val_accuracy: 0.5589\n",
      "Epoch 10/10\n",
      "1571/1571 [==============================] - 15s 9ms/step - loss: 0.6813 - accuracy: 0.5631 - val_loss: 0.6838 - val_accuracy: 0.5532\n",
      "Test loss: 0.6885271668434143\n",
      "Test accuracy: 0.5344794988632202\n"
     ]
    }
   ],
   "source": [
    "# 10 epochs\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(22, activation='relu'))\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x, np.array(train_y),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    validation_data=(validation_x, np.array(validation_y))\n",
    ")\n",
    "    # callbacks=[tensorboard, checkpoint],\n",
    "# )\n",
    "\n",
    "# Score model\n",
    "score = model.evaluate(test_x, np.array(test_y), verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# Save model\n",
    "# model.save(PATH+\"/models/{}\".format(NAME))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tuned LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
